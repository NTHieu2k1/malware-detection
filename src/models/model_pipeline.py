import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from src.visualization.explorers import SampleExplorer, BenignExplorer, MalwareExplorer
from src.data.preprocessors import DataPreprocessor
from src.features.selectors import RandomForestSelector
from src.models.train_dev_test_split import TrainDevTestSplit
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import recall_score, fbeta_score, ConfusionMatrixDisplay, make_scorer
from sklearn.model_selection import RandomizedSearchCV
from src.utilities.data import merge_dataframes, check_imbalanced_dataset
from src.utilities.file import get_project_root_directory
from pathlib import Path


class RandomForestMalwareDetector:
    def __init__(self):
        self._seed = 35
        self.benign_explorer = BenignExplorer()
        self.malware_explorer = MalwareExplorer()
        self.sample_explorer = SampleExplorer()
        self.preprocessor = DataPreprocessor()
        self.feature_selector = RandomForestSelector()
        self.train_dev_test_split = TrainDevTestSplit()
        self.smote = SMOTE(random_state=self._seed)
        self._scaler = MinMaxScaler()
        self._random_forest_model = RandomForestClassifier(n_jobs=-1, random_state=self._seed)
        self.scaled_random_forest_model = Pipeline([('scaler', self._scaler), ('random forest model', self._random_forest_model)])
        self.dev_set_numpy_path = get_project_root_directory() / Path('data/processed/dev_set.npy')
        self.test_set_numpy_path = get_project_root_directory() / Path('data/processed/test_set.npy')
        self._is_imbalanced_dataset = None
        self._scoring_metrics = {'recall': make_scorer(recall_score), 'f2': make_scorer(fbeta_score, beta=2)}

    def _explore_data(self) -> pd.DataFrame:
        self.benign_explorer.get_all_benign_data()
        self.malware_explorer.get_all_malware_data()
        raw_data = merge_dataframes([self.benign_explorer.benign_data, self.malware_explorer.malware_data])
        return raw_data

    def _preprocess_data(self, raw_data: pd.DataFrame) -> pd.DataFrame:
        self.preprocessor.import_data(raw_data)
        self.preprocessor.shuffle_data()
        self.preprocessor.remove_useless_features()
        self.preprocessor.label_encoder()
        self.preprocessor.drop_nan_samples()
        preprocessed_data = self.preprocessor.data
        return preprocessed_data

    def _feature_selection(self, preprocessed_data: pd.DataFrame) -> pd.DataFrame:
        self.feature_selector.import_data(preprocessed_data)
        self.feature_selector.train_selector()
        self.feature_selector.select_features()
        self.feature_selector.transform()
        final_data = self.feature_selector.final_data
        return final_data

    def _train_dev_test_split(self, final_data: pd.DataFrame, dev_size: float, test_size: float):
        self.train_dev_test_split.import_data(final_data)
        return self.train_dev_test_split.train_dev_test_split(test_size, dev_size)

    @staticmethod
    def _save_to_npy(feat_data: np.ndarray, class_data: np.ndarray, file_location):
        with open(file_location, 'wb') as file_save:
            np.save(file_save, feat_data)
            np.save(file_save, class_data)

    def train_model(self, dev_size: float, test_size: float):
        # Explore all raw samples for training, and put them into a dataframe
        raw_data = self._explore_data()
        # Pre-process the data
        preprocessed_data = self._preprocess_data(raw_data)
        # Feature selection
        final_data = self._feature_selection(preprocessed_data)
        # Check whether the dataset is imbalanced (means one class is more than 1.5 times larger than the others)
        is_imbalanced_dataset = check_imbalanced_dataset(final_data)
        # Train-dev-test split
        train_feat, dev_feat, test_feat, train_class, dev_class, test_class = self._train_dev_test_split(final_data,
                                                                                                         dev_size,
                                                                                                         test_size)
        # Save validation (dev) data to a numpy (.npy) file
        self._save_to_npy(dev_feat, dev_class, self.dev_set_numpy_path)
        # Save test data to an another numpy (.npy) file
        self._save_to_npy(test_feat, test_class, self.test_set_numpy_path)
        # If the original dataset is imbalanced, over-sampling the training data (using SMOTE),
        # to make training data balanced
        if is_imbalanced_dataset:
            train_feat_resampled, train_class_resampled = self.smote.fit_resample(train_feat, train_class)
            # Train the Scaled Random Forest model
            self.scaled_random_forest_model.fit(train_feat_resampled, train_class_resampled)
        # Otherwise, just put training data for training the model
        # (no need for over-sampling in well-balanced dataset)
        else:
            self.scaled_random_forest_model.fit(train_feat, train_class)

    @staticmethod
    def _load_data_from_npy(file_location):
        with open(file_location, 'rb') as file_load:
            feat_data = np.load(file_load)
            class_data = np.load(file_load)
        return feat_data, class_data

    @staticmethod
    def _display_metrics(evaluation_type: str, ground_truth, predictions):
        print('({0}) Recall:'.format(evaluation_type), end=' ')
        print('%.2f%%' % (recall_score(ground_truth, predictions) * 100))
        print('({0}) F2-score:'.format(evaluation_type), end=' ')
        print('%.2f%%' % (fbeta_score(ground_truth, predictions, beta=2) * 100), end='\n\n')

    @staticmethod
    def _display_confusion_matrix(title: str, ground_truth, predictions, labels=[1, 0], cmap=plt.cm.Blues):
        ConfusionMatrixDisplay.from_predictions(ground_truth, predictions, labels=labels, cmap=cmap)
        plt.title(title)
        plt.show()

    def evaluate_model(self):
        # Load dev set from numpy file
        dev_feat, dev_class = self._load_data_from_npy(self.dev_set_numpy_path)
        # Load test set from numpy file
        test_feat, test_class = self._load_data_from_npy(self.test_set_numpy_path)
        # Use the trained model to predict samples from dev & test sets
        dev_predictions = self.scaled_random_forest_model.predict(dev_feat)
        test_predictions = self.scaled_random_forest_model.predict(test_feat)
        # Display evaluation metrics' results
        self._display_metrics('Test', test_class, test_predictions)
        self._display_metrics('Validation', dev_class, dev_predictions)
        # Display confusion matrix of dev set and test set
        self._display_confusion_matrix('Testing Results', test_class, test_predictions)
        self._display_confusion_matrix('Validation Results', dev_class, dev_predictions)

    @staticmethod
    def _is_missing_values(dataframe: pd.DataFrame) -> bool:
        nan_status_df = dataframe.isna()
        nan_status_array = np.asarray(nan_status_df)[0]
        return any(nan_status_array)

    def _predict_from_dataframe(self, dataframe: pd.DataFrame) -> dict:
        features_data = np.asarray(dataframe)
        probabilities = self.scaled_random_forest_model.predict_proba(features_data)[0]
        positive_probability = probabilities[1]
        negative_probability = probabilities[0]
        prediction_results = {
            'benign': '%.2f%%' % (negative_probability*100),
            'malware': '%.2f%%' % (positive_probability*100)
        }
        return prediction_results

    def predict_a_sample(self, sample_content: bytes):
        # Get all PE headers of the sample
        pe_headers = self.sample_explorer.get_all_pe_information(pe_file_content=sample_content)
        pe_data = pd.DataFrame.from_dict(pe_headers)
        # Feature selection (derived from trained model)
        processed_data = pe_data[self.feature_selector.selected_features]
        # Raise error if there are NaN values in the data
        if self._is_missing_values(processed_data):
            raise ValueError('Missing values encountered in this sample.')
        return self._predict_from_dataframe(processed_data)

    @staticmethod
    def _concatenate_train_with_dev_set(train_feat, train_class, dev_feat, dev_class):
        combined_train_dev_feat = np.concatenate([train_feat, dev_feat], axis=0)
        combined_train_dev_class = np.concatenate([train_class, dev_class])
        return combined_train_dev_feat, combined_train_dev_class

    @staticmethod
    def _create_hyperparameter_ranges():
        n_estimators = [int(estimators) for estimators in range(10, 201, 10)]
        max_depth = [int(depth) for depth in range(20, 151, 10)]
        max_depth.append(None)
        min_samples_split = [int(no_samples_split) for no_samples_split in range(2, 11)]
        min_samples_leaf = [int(no_samples_leaf) for no_samples_leaf in range(1, 6)]
        hyperparam_ranges = {
            'random forest model__n_estimators': n_estimators,
            'random forest model__max_depth': max_depth,
            'random forest model__min_samples_split': min_samples_split,
            'random forest model__min_samples_leaf': min_samples_leaf
        }
        return hyperparam_ranges

    @staticmethod
    def _display_tuning_score(model_tuner):
        print('Tuning completed.')
        print('Recall score after tuning: %.2f%%\n' % (model_tuner.best_score_*100))

    def fine_tune_model(self):
        # Temporarily set scoring metrics
        score_metrics = {'recall': make_scorer(recall_score), 'f2': make_scorer(fbeta_score, beta=2)}
        self._scoring_metrics = score_metrics
        # Temporarily set seed value
        self._seed = 35
        # Temporarily set all modifications in the model
        self.smote = SMOTE(random_state=self._seed)
        self._random_forest_model = RandomForestClassifier(n_jobs=-1, random_state=self._seed)
        self.scaled_random_forest_model = Pipeline([('scaler', self._scaler),
                                                    ('random forest model', self._random_forest_model)])
        # Retrieve original training set
        original_training_set = self.train_dev_test_split.train_set
        original_train_feat = np.asarray(original_training_set.drop(['Class'], axis=1))
        original_train_class = np.asarray(original_training_set['Class'])
        # Get "is imbalanced dataset" to check whether the dataset is imbalanced
        self._is_imbalanced_dataset = check_imbalanced_dataset(original_training_set)
        # If the dataset is imbalanced, over-sampling the training set
        if self._is_imbalanced_dataset:
            resampled_train_feat, resampled_train_class = self.smote.fit_resample(original_train_feat, original_train_class)
            train_feat, train_class = resampled_train_feat, resampled_train_class
        else:
            train_feat, train_class = original_train_feat, original_train_class
        # Get dev set from numpy file
        dev_feat, dev_class = self._load_data_from_npy(self.dev_set_numpy_path)
        # Concatenate training set with dev set
        combined_train_dev_feat, combined_train_dev_class = self._concatenate_train_with_dev_set(train_feat, train_class,
                                                                                                 dev_feat, dev_class)
        # Create hyperparameter ranges (which would be used for tuning)
        hyperparam_ranges = self._create_hyperparameter_ranges()
        # Use Randomized Search CV for tuning the model
        model_tuner = RandomizedSearchCV(self.scaled_random_forest_model, param_distributions=hyperparam_ranges,
                                         n_iter=500, scoring=self._scoring_metrics, cv=10, refit='recall')
        model_tuner.fit(combined_train_dev_feat, combined_train_dev_class)
        # Display score metrics
        self._display_tuning_score(model_tuner)
        # Apply well-tuned model as the new main model
        self.scaled_random_forest_model = model_tuner.best_estimator_
