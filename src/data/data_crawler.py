import os
import asyncio
from pyppeteer import launch, page
import math
import re
from pathlib import Path

raw_data_project_directory = 'data/raw'


def get_project_root_directory():
    data_source_directory = Path(__file__)
    project_root_directory = data_source_directory.parent.parent.parent
    return project_root_directory


def get_raw_data_directory():
    project_root_dir = get_project_root_directory()
    raw_data_dir = os.path.join(project_root_dir, Path(raw_data_project_directory))
    return raw_data_dir


async def launch_browser():
    return await launch({'headless': False})


async def open_page(browser, url) -> page.Page:
    browser_page = await browser.newPage()
    await browser_page.goto(url)
    return browser_page


async def get_categories_n_urls(browser) -> dict:
    # Open taimienphi website
    tmp_page = await open_page(browser, url='https://taimienphi.vn')

    # Select "a" tag that contains categories and their urls
    category_url_xpath = '/html[1]/body[1]/form[1]/div[4]/div[2]/div[3]/div[1]/ul[1]/li/a[1]'
    elements = await tmp_page.xpath(category_url_xpath)
    link_elements = dict()

    # Extract urls and categories themselves
    for element in elements:
        link = await tmp_page.evaluate('(element) => element.href', element)
        folder = await tmp_page.evaluate('(element) => element.textContent', element)
        link_elements.update({folder: link})

    await tmp_page.close()
    return link_elements


def preprocessing_category_url_dictionary(url_data: dict) -> dict:
    """This function is used to pre-process URL-category data crawled in taimienphi website.
    This function consisted of 2 parts: remove some unnecessary categories,
    and replacing whitespaces in names.
    """

    # Define unnecessary categories
    unnecessary_categories = [
        'Mới cập nhật',
        'Top Tải về',
        'Tài liệu',
        'Biểu mẫu',
        'Đề thi',
        'Học ngoại ngữ',
        'Ứng dụng Android',
        'Ứng dụng iOS',
        'Gọi xe',
        'Vận tải, hành khách',
        'Phần mềm mới'
    ]

    for category in list(url_data.keys()):
        # Remove some unnecessary categories
        if category in unnecessary_categories:
            url_data.pop(category)
            continue
        # Other categories: replace whitespaces by underscores
        fixed_category = category.replace(' ', '_')
        # Then apply back to data
        url_data[fixed_category] = url_data.pop(category)

    # Return preprocessed data
    return url_data


async def test_run_page(browser, url):
    """Just testing running a page."""
    # Open the page
    new_page = await browser.newPage()
    await new_page.goto(url)

    # Keep open for 2 seconds
    await asyncio.sleep(2)

    # Close the page
    await new_page.close()


def group_splitter(element_list: list, no_of_items: int = 2):
    """This function is for splitting the list into groups.
    Default: 2 items in each group.
    """
    total_items = len(element_list)
    no_of_groups = math.ceil(total_items / no_of_items)

    for _ in range(no_of_groups):
        group = list()
        group.clear()
        for _ in range(no_of_items):
            if type(element_list[0]) is not list:
                group.append(element_list.pop(0))
        element_list.append(group)


def make_subdirectories(root_dir, list_subdirs: list) -> list:
    """Make directories for categories and sub-categories,
    as well as creating their paths.
    """
    sub_paths = [os.path.join(root_dir, subdir) for subdir in list_subdirs]
    for sub_path in sub_paths:
        if not Path(sub_path).exists():
            os.mkdir(sub_path)
    return sub_paths


async def retrieve_subcategory_urls(browser, url) -> list:
    """Retrieve list of URLs (corresponding to sub-categories)
    for each category.
    """

    # Open new category page
    cat_page = await open_page(browser, url)
    await cat_page.goto(url)

    # Retrieve nodes that contains sub-category's URLs
    subcategory_url_xpath = '/html[1]/body[1]/form[1]/div[4]/div[3]/div[2]/ul[1]/li/a[1]'
    url_nodes = await cat_page.xpath(subcategory_url_xpath)

    # Retrieve URLs from those nodes
    sub_cat_urls = list()
    for node in url_nodes:
        sub_url = await cat_page.evaluate('(node) => node.href', node)
        sub_cat_urls.append(sub_url)

    # Close category page
    await cat_page.close()
    return sub_cat_urls


async def get_num_of_apps(sub_cat_page) -> int:
    num_apps_xpath = '/html/body/form/div[4]/div[3]/h2/span'
    num_apps_element = await sub_cat_page.xpath(num_apps_xpath)
    node = num_apps_element[0]
    raw_string = await sub_cat_page.evaluate('(node) => node.textContent', node)
    num_apps = int(re.findall('[0-9]+', raw_string)[0])
    return num_apps


async def retrieve_app_urls(app_page) -> list:
    app_url_xpath = '/html[1]/body[1]/form[1]/div[4]/div[3]/div[6]/table[1]/tbody[1]/tr/td[4]/a[1]'
    app_nodes = await app_page.xpath(app_url_xpath)
    app_urls = list()
    for node in app_nodes:
        app_url = await app_page.evaluate('(node) => node.href', node)
        app_urls.append(app_url)
    return app_urls


async def subcategory_processing(cat_browser, sub_cat_url):
    # Open new sub-category page
    sub_cat_page = await open_page(cat_browser, sub_cat_url)

    # Get total number of apps in the sub-category
    total_apps_num = await get_num_of_apps(sub_cat_page)
    num_of_pages = math.ceil(total_apps_num / 10)
    await sub_cat_page.close()

    # Process each page and get 10 app URLs of each
    for page_num in range(1, num_of_pages+1):
        page_url = sub_cat_url + '/' + str(page_num)
        app_page = await open_page(cat_browser, page_url)
        app_urls = await retrieve_app_urls(app_page)
        await app_page.close()
        # Just testing open 10 app URLs
        await asyncio.gather(*[test_run_page(cat_browser, app_url) for app_url in app_urls])


async def category_processing(cat_url):
    # Open new window for new category
    cat_browser = await launch_browser()

    # Retrieve list of URLs of sub-categories
    sub_cat_urls = await retrieve_subcategory_urls(cat_browser, cat_url)

    # Processing every sub-categories
    for sub_cat_url in sub_cat_urls:
        await subcategory_processing(cat_browser, sub_cat_url)

    # Close the window when processing complete
    await cat_browser.close()


async def main():
    """The heart of the software crawler."""

    # Open new browser window, and get the url-category data
    init_browser = await launch_browser()
    link_dict = await get_categories_n_urls(init_browser)

    # Preprocessing the url-category data, and get URL and categories data
    link_dict = preprocessing_category_url_dictionary(link_dict)
    url_list = list(link_dict.values())
    cat_list = list(link_dict.keys())

    # Create folders for corresponding categories, and get their paths
    raw_data_dir = get_raw_data_directory()
    sub_paths = make_subdirectories(raw_data_dir, cat_list)

    # Close the init window
    await init_browser.close()

    # Split categories into groups
    group_splitter(url_list)

    # Let each category to run into new windows, and concurrently processing (in each group)
    for url_group in url_list:
        await asyncio.gather(*[category_processing(cat_url) for cat_url in url_group])


asyncio.run(main())
