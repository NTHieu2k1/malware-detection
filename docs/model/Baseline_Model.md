# Baseline Model Report

This report contains analytic approach for building model and model pipeline,
as every model use the same pipeline for training.

## 1. Analytic approach:

Mainly, Random Forest method was used for training model. This method
consists of various of decision trees, and the outcome is calculated
based on predictions of those trees.

However, as the dataset maybe imbalanced between malware and benign
classes, an oversampling method, called SMOTE, was used. SMOTE stands
for Synthetic Minority Over-sampling Technique. Moreover, this method is
applied in the training set, in order to make classes balanced synthetically.

## 2. Model Pipeline:

At first, the dataset was put in a data pipeline for pre-processing,
before putting in training. Details of data pipeline is in
![Data Pipeline Report](/docs/data/Data_Pipeline.md).

Here are some parts of model pipeline:

* **Train/dev/test split**: The dataset was split balancedly into training,
dev and test sets, with given dev set size and test set size, to make
proportion of classes on each set are equal to the original one. To
split balancedly, the dataset was split into 2 different sets (malware
and benign sets), then each set was continuously split into 3 subsets
(train, dev, test) of each; finally, combine all train, dev, test subsets
of each class into one training set, one dev set, and one test set.
This ensured proportion of classes on 3 sets after splitting remained
the same as the original dataset.
* **Check imbalanced dataset**: Check whether the dataset is imbalanced
between classes by calculating ratio between classes. The dataset is skewed
if one class is over 1.5 times bigger than the others.
* **Oversampling**: If the dataset is imbalanced, SMOTE method is used
to make classes balanced (proportion of each is the same), in the training
set. This method may reduce bias that may be generated by imbalanced
data during training.
* **Feature Scaling**: Min-max scaling was applied in all training, dev
and test sets. This scaled all ranges of features into 0 - 1 range, in
order to boost up training process and reduce computational cost.
* **Random Forest model**: Training set after scaling was put into a
Random Forest model, and used for training the model.
