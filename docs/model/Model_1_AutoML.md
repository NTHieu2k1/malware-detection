# Model 1 AutoML Report

This is a report of a model which was trained via an ML automation pipeline
(using PyCaret library).

## 1. Training approach:

After comparing based models and testing, **Random Forest - Naive Bayes
combination** approach was chosen. Detailed information about why this
approach was chosen, is in the PyCaret uses notebook [here](/notebooks/PyCaret uses.ipynb).

## 2. Training steps:

- __Setup environment__: Enable preprocessing with label encoding and 
pass all features as numeric feature (because some of them are recognized
as categorical, not numeric themselves); feature selection; train-test
split (with 15% dataset for test set); enable min-max scaling and fix
imbalanced issue.
- __Train standalone models__: Train and tune a Random Forest model, and
a Naive Bayes model respectively.
- __Combine models__: Use blending method to combine those two models
to a single one, then evaluate it.
- __Finalize combination__: Finalize the combined model for deployment.

## 3. Evaluation results:

* Recall: 97.74%
* F2 score: 73.77%
* Confusion matrix:

![Confusion Matrix](/docs/model/Model_1_AutoML/Confusion_Matrix.png)

In the confusion matrix, rows indicate true labels, and columns indicate
predicted ones. Moreover, label "0" means benign, and "1" means malware.
As a result, the matrix shows that only 5 malware samples were mis-detected,
whereas number of mistaken-alerted samples is 364.

* Learning curve:

![Learning Curve](/docs/model/Model_1_AutoML/Learning_Curve.png)

The curve shows that training and validation curves are similar to each
other, which means the model is absolutely not overfitting and able to
generalize well on test set.

## 4. Test results on deployment:

To check the performance of the model in real-world, the model was deployed
to the API which was built before. And Postman API platform was used to
test the model with a small, prepared dataset.

### a. Data preparation:

The small dataset consists of total of 40 PE samples, includes:

- 20 goodwares, which were crawled from [CNET download page](https://download.cnet.com/).
- 20 malwares, which were crawled from [Any.Run sandbox](https://app.any.run/).

The test case was organized as follows:

- The first 20 samples are goodwares (benign).
- The last 20 samples are malwares.

### b. Testing results:

First, all samples were uploaded orderly into the API. Then, they were
successfully put into prediction and enqueued.

After finishing predictions, results testing was implemented via Newman.
Here is the results.

![Samples 1 to 5](/docs/model/Model_1_AutoML_Deploy_Results/Samples_1_to_5.png)

![Samples 6 to 10](/docs/model/Model_1_AutoML_Deploy_Results/Samples_6_to_10.png)

![Samples 11 to 15](/docs/model/Model_1_AutoML_Deploy_Results/Samples_11_to_15.png)

![Samples 16 to 20](/docs/model/Model_1_AutoML_Deploy_Results/Samples_16_to_20.png)

![Samples 21 to 25](/docs/model/Model_1_AutoML_Deploy_Results/Samples_21_to_25.png)

![Samples 26 to 30](/docs/model/Model_1_AutoML_Deploy_Results/Samples_26_to_30.png)

![Samples 31 to 35](/docs/model/Model_1_AutoML_Deploy_Results/Samples_31_to_35.png)

![Samples 36 to 40](/docs/model/Model_1_AutoML_Deploy_Results/Samples_36_to_40.png)

Test report:

![Test report](/docs/model/Model_1_AutoML_Deploy_Results/Test_report.png)

The report shows that 15 samples were mis-classified, that means only
55% of samples were correctly classified. Following is the detailed
information about those errors.

![Error info 1 to 7](/docs/model/Model_1_AutoML_Deploy_Results/Error_details_1_to_7.png)

![Error info 8 to 15](/docs/model/Model_1_AutoML_Deploy_Results/Error_details_8_15.png)

The detailed information indicates that 11 of 15 mis-classified samples
were goodwares, and only 4 were malwares. As a result, the model only
mis-detected 4 malwares (20% of all malwares) although mistakenly
alerted 11 benign samples.

__In summary__, the model is able to detect malwares with high accuracy,
as well as work well on real world data.
