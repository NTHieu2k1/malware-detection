import asyncio
from pyppeteer import launch, page
import math
import re
from pathlib import Path

raw_data_project_directory = 'data/raw'


def get_project_root_directory():
    data_source_directory = Path().resolve()
    project_root_directory = data_source_directory.parent.parent
    return project_root_directory


def get_raw_data_directory():
    project_root_dir = Path(get_project_root_directory())
    raw_data_dir = project_root_dir/Path(raw_data_project_directory)
    return raw_data_dir


async def launch_browser():
    return await launch({'headless': False, 'args': ['--no-sandbox', '--disable-dev-shm-usage'],
                         'waitUntil': 'networkidle0'})


async def open_page(browser, url, timeout=90000) -> page.Page:
    browser_page = await browser.newPage()
    await browser_page.goto(url, {'timeout': timeout})
    return browser_page


async def get_categories_n_urls(browser) -> dict:
    # Open taimienphi website
    tmp_page = await open_page(browser, url='https://taimienphi.vn')

    # Select "a" tag that contains categories and their urls
    category_url_xpath = '/html[1]/body[1]/form[1]/div[4]/div[2]/div[3]/div[1]/ul[1]/li/a[1]'
    elements = await tmp_page.xpath(category_url_xpath)
    link_elements = dict()

    # Extract urls and categories themselves
    for element in elements:
        link = await tmp_page.evaluate('(element) => element.href', element)
        folder = await tmp_page.evaluate('(element) => element.textContent', element)
        link_elements.update({folder: link})

    await tmp_page.close()
    return link_elements


def remove_unnecessary_categories(category_url_data, unnecessary_categories: list):
    for category in list(category_url_data.keys()):
        if category in unnecessary_categories:
            category_url_data.pop(category)


def replace_space_in_categories(category_url_data: dict):
    for category in list(category_url_data.keys()):
        # Replace space in categories
        fixed_category = category.replace(' ', '_')
        # Apply back to category-url dictionary
        category_url_data[fixed_category] = category_url_data.pop(category)


def preprocessing_category_url_dictionary(category_url_data: dict) -> dict:
    """This function is used to pre-process category-URL data crawled in taimienphi website.
    This function consisted of 2 parts: remove some unnecessary categories,
    and replacing whitespaces in names.
    """

    # Define unnecessary categories
    unnecessary_categories = [
        'Mới cập nhật',
        'Top Tải về',
        'Tài liệu',
        'Biểu mẫu',
        'Đề thi',
        'Học ngoại ngữ',
        'Ứng dụng Android',
        'Ứng dụng iOS',
        'Gọi xe',
        'Vận tải, hành khách',
        'Phần mềm mới'
    ]

    # Remove unnecessary categories
    remove_unnecessary_categories(category_url_data, unnecessary_categories)

    # Replace whitespace in names (categories)
    replace_space_in_categories(category_url_data)

    # Return preprocessed data
    return category_url_data


def group_splitter(element_list: list, no_of_items: int = 2) -> list:
    """This function is for splitting the list into groups.
    Default: 2 items in each group.
    """
    total_items = len(element_list)
    new_group_list = list()
    for item in range(0, total_items, no_of_items):
        group = element_list[item: item+no_of_items]
        new_group_list.append(group)
    return new_group_list


def make_subdirectories(root_dir, list_subdirs: list) -> list:
    """Make directories for categories and sub-categories,
    as well as creating their paths.
    """
    sub_paths = [Path(root_dir)/Path(subdir) for subdir in list_subdirs]
    for sub_path in sub_paths:
        if not Path(sub_path).exists():
            Path(sub_path).mkdir(parents=True, exist_ok=True)
    return sub_paths


async def retrieve_subcategory_urls(browser, url) -> list:
    """Retrieve list of URLs (corresponding to sub-categories)
    for each category.
    """

    # Open new category page
    cat_page = await open_page(browser, url)
    await cat_page.goto(url)

    # Retrieve nodes that contains sub-category's URLs
    subcategory_url_xpath = '/html[1]/body[1]/form[1]/div[4]/div[3]/div[2]/ul[1]/li/a[1]'
    url_nodes = await cat_page.xpath(subcategory_url_xpath)

    # Retrieve URLs from those nodes
    sub_cat_urls = list()
    for node in url_nodes:
        sub_url = await cat_page.evaluate('(node) => node.href', node)
        sub_cat_urls.append(sub_url)

    # Close category page
    await cat_page.close()
    return sub_cat_urls


async def get_num_of_apps(sub_cat_page) -> int:
    num_apps_xpath = '/html/body/form/div[4]/div[3]/h2/span'
    num_apps_element = await sub_cat_page.xpath(num_apps_xpath)
    node = num_apps_element[0]
    raw_string = await sub_cat_page.evaluate('(node) => node.textContent', node)
    num_apps = int(re.findall('[0-9]+', raw_string)[0])
    return num_apps


async def retrieve_app_urls(app_page) -> list:
    app_url_xpath = '/html[1]/body[1]/form[1]/div[4]/div[3]/div/table[1]/tbody[1]/tr/td[4]/a[1]'
    app_nodes = await app_page.xpath(app_url_xpath)
    app_urls = list()
    for node in app_nodes:
        app_url = await app_page.evaluate('(node) => node.href', node)
        app_urls.append(app_url)
    return app_urls


async def subcategory_processing(cat_browser, sub_cat_url, category_path):
    # Open new sub-category page
    sub_cat_page = await open_page(cat_browser, sub_cat_url)

    # Get total number of apps in the sub-category
    total_apps_num = await get_num_of_apps(sub_cat_page)
    num_of_pages = math.ceil(total_apps_num / 10)
    await sub_cat_page.close()

    # Process each page and get 10 app URLs of each
    for page_num in range(1, num_of_pages+1):
        page_url = sub_cat_url + '/' + str(page_num)
        app_page = await open_page(cat_browser, page_url)
        app_urls = await retrieve_app_urls(app_page)
        await app_page.close()
        # Process and download 10 apps
        await asyncio.gather(*[process_n_download_app_data(cat_browser, app_url, category_path)
                               for app_url in app_urls])


async def retrieve_os_info(app_page):
    os_xpath = '/html/body/form/div[4]/div/div[1]/ul/li[7]/span'
    os_nodes = await app_page.xpath(os_xpath)
    os_node = os_nodes[0]
    os_info = await app_page.evaluate('(os_node) => os_node.textContent', os_node)
    return os_info


async def get_download_url_n_file_name(app_page):
    """Retrieve downloadable URL and download file name of the app."""
    download_file_xpath = '/html/body/form/div[4]/div/div/div[4]/div[1]/div/a'
    download_file_nodes = await app_page.xpath(download_file_xpath)
    download_file_node = download_file_nodes[0]
    download_url = await app_page.evaluate('(download_file_node) => download_file_node.href',
                                           download_file_node)
    file_name = download_url.split('/')[-1]
    return download_url, file_name


def is_file_exist(category_path, file_name) -> bool:
    file_path = Path(category_path)/Path(file_name)
    return Path(file_path).exists()


async def download_app(app_page, download_url, is_file_exists: bool):
    """Only download file if the file is not exist in the directory.
    This avoids re-downloading files that leads to duplicate files.
    """
    if not is_file_exists:
        try:
            await app_page.goto(download_url)
            await asyncio.sleep(10)
        except Exception as e:
            # Set download timeout for 120 seconds
            await app_page.waitFor(120000)


async def process_n_download_app_data(category_browser, app_url, category_path):
    """Retrieving app information, and download app files."""

    # Open the application page
    app_page = await open_page(category_browser, app_url)
    
    # Set download location to the category path
    await app_page._client.send('Page.setDownloadBehavior', {'behavior': 'allow',
                                                             'downloadPath': str(category_path)})

    # Retrieve supporting OS(es) of the app
    support_os = await retrieve_os_info(app_page)

    # Retrieve download URL of the app, as well as its file name
    download_url, file_name = await get_download_url_n_file_name(app_page)

    # Generate Windows OS supporting flag
    windows_support_flag = support_os.find('Windows') > -1

    # Only download apps that supports Windows OS
    if windows_support_flag:
        file_exists = is_file_exist(category_path, file_name)
        await download_app(app_page, download_url, file_exists)

    # Download complete. Close the page
    await app_page.close()


async def category_processing(cat_url, category_path):
    # Open new window for new category
    cat_browser = await launch_browser()

    # Retrieve list of URLs of sub-categories
    sub_cat_urls = await retrieve_subcategory_urls(cat_browser, cat_url)

    # Processing every sub-categories
    for sub_cat_url in sub_cat_urls:
        await subcategory_processing(cat_browser, sub_cat_url, category_path)

    # Close the window when processing complete
    await cat_browser.close()


async def main():
    """The heart of the software crawler."""

    # Open new browser window, and get the category-url data
    init_browser = await launch_browser()
    category_n_url_dict = await get_categories_n_urls(init_browser)

    # Preprocessing the category-url data, and get URL and categories data
    category_n_url_dict = preprocessing_category_url_dictionary(category_n_url_dict)
    url_list = list(category_n_url_dict.values())
    category_list = list(category_n_url_dict.keys())

    # Create folders for corresponding categories, and get their paths
    raw_data_dir = get_raw_data_directory()
    category_paths = make_subdirectories(raw_data_dir, category_list)

    # Close the init window
    await init_browser.close()

    # Split categories into groups
    url_groups = group_splitter(url_list)
    category_groups = group_splitter(category_paths)

    # Let each category to run into new windows, and concurrently processing (in each group)
    for url_group, category_group in zip(url_groups, category_groups):
        await asyncio.gather(*[category_processing(cat_url, category_path)
                               for cat_url, category_path in zip(url_group, category_group)])


asyncio.run(main())
