# Model 1 Tuning Report

## 1. Tuning procedures:

### a. Hyperparameters selected for tuning, and ranges:

- n_estimators (range 100 - 250)
- max_depth (range 20 - 150)
- min_samples_split (range 2 - 10)
- min_samples_leaf (range 1 - 5)

All hyperparameters are from the Random Forest model.

### b. Tools (libraries) for tuning:

Tuning was done by using Optuna library.

### c. Steps to tune hyperparameters:

- Retrieve training and dev sets, and combine them.
- Define hyperparameters' ranges for later tuning.
- Create an Optuna's Study instance for tuning.
- Put the model needed to tune and dataset into cross-validation process.
- Use the Study instance to iterate between hyperparameter ranges and
optimize the cross-validation, to select the best hyperparameters.
- Set the model with the best hyperparameters found.

## 2. Tuning results:

### a. After tuning:

* Recall: 97.11%

### b. Evaluation using test set:

* Recall: 93.39%
* F2-score: 92.09%
* Confusion matrix:

![Tuning Confusion Matrix](/docs/model/Model_1_Tuning/Tuning_confusion_matrix.png)

It seems that although best parameters were chosen, performance of the
tuned model is slightly worse than the initially-trained one (recall score
is lower by 1.5%).

## 3. Boosting the model:

### a. Boosting method:

Gradient Boosting method is used for boosting the model. A booster using
this method was defined with "n_estimators" parameter equals 100. Then,
training data was put to the booster model.

### b. Boosting results:

* Recall: 93.83%
* F2-score: 87.8%
* Confusion matrix:

![Boosting Confusion Matrix](/docs/model/Model_1_Tuning/Boosting_confusion_matrix.png)

Recall score of the model after boosting slightly rose by 0.5% compared
to the model before boosting. However, the performance is still just
worse than the initially-trained model.
