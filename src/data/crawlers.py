import asyncio
from pyppeteer import launch
import math
import re
from pathlib import Path


class TaiMienPhiCrawler:
    def __init__(self, headless, args):
        self.raw_data_project_directory = 'data/raw'
        self._taimienphi_website = 'https://taimienphi.vn'
        self._timeout = 90000
        self._browser_params = {'headless': headless, 'args': args, 'waitUntil': 'networkidle0'}
        self._category_url_dict = dict()

    @property
    def category_url_dict(self):
        return self._category_url_dict

    @category_url_dict.setter
    def category_url_dict(self, new_category_url_dict):
        self._category_url_dict = new_category_url_dict

    @staticmethod
    def get_project_root_directory():
        data_source_directory = Path(__file__)
        project_root_directory = data_source_directory.parent.parent.parent
        return project_root_directory

    def get_raw_data_directory(self):
        project_root_dir = self.get_project_root_directory()
        raw_data_dir = project_root_dir/Path(self.raw_data_project_directory)
        return raw_data_dir

    async def launch_browser(self):
        return await launch(self._browser_params)

    async def open_page(self, browser, url):
        browser_page = await browser.newPage()
        await browser_page.goto(url, {'_timeout': self._timeout})
        return browser_page

    async def get_categories_n_urls(self, browser):
        # Open taimienphi website
        tmp_page = await self.open_page(browser, self._taimienphi_website)
        # Select "a" tag that contains categories and their urls
        category_url_xpath = '/html[1]/body[1]/form[1]/div[4]/div[2]/div[3]/div[1]/ul[1]/li/a[1]'
        elements = await tmp_page.xpath(category_url_xpath)
        link_elements = dict()
        # Extract urls and categories themselves
        for element in elements:
            link = await tmp_page.evaluate('(element) => element.href', element)
            folder = await tmp_page.evaluate('(element) => element.textContent', element)
            link_elements.update({folder: link})
        self.category_url_dict = link_elements
        await tmp_page.close()

    @staticmethod
    def remove_unnecessary_categories(category_url_data):
        # Define unnecessary categories
        unnecessary_categories = [
            'Mới cập nhật',
            'Top Tải về',
            'Tài liệu',
            'Biểu mẫu',
            'Đề thi',
            'Học ngoại ngữ',
            'Ứng dụng Android',
            'Ứng dụng iOS',
            'Gọi xe',
            'Vận tải, hành khách',
            'Phần mềm mới'
        ]
        for category in list(category_url_data.keys()):
            # Remove unnecessary categories
            if category in unnecessary_categories:
                category_url_data.pop(category)

    @staticmethod
    def replace_space_in_categories(category_url_data):
        for category in list(category_url_data.keys()):
            # Replace space in categories
            fixed_category = category.replace(' ', '_')
            # Apply back to category-url dictionary
            category_url_data[fixed_category] = category_url_data.pop(category)

    def preprocessing_category_url_dictionary(self):
        """This function is used to pre-process category-URL data crawled in taimienphi website.
        This function consisted of 2 parts: remove some unnecessary categories,
        and replacing whitespaces in names.
        """
        category_url_data = self.category_url_dict
        # Remove unnecessary categories
        self.remove_unnecessary_categories(category_url_data)
        # Replace whitespace in names (categories)
        self.replace_space_in_categories(category_url_data)
        self.category_url_dict = category_url_data

    @staticmethod
    def make_subdirectories(root_dir, list_subdirs: list) -> list:
        """Make directories for categories and sub-categories,
        as well as creating their paths.
        """
        sub_paths = [Path(root_dir)/Path(subdir) for subdir in list_subdirs]
        for sub_path in sub_paths:
            Path(sub_path).mkdir(parents=True, exist_ok=True)
        return sub_paths

    async def retrieve_subcategory_urls(self, category_browser, url) -> list:
        """Retrieve list of URLs (corresponding to sub-categories)
        for each category.
        """
        # Open new category page
        cat_page = await self.open_page(category_browser, url)
        await cat_page.goto(url)
        # Retrieve nodes that contains sub-category's URLs
        subcategory_url_xpath = '/html[1]/body[1]/form[1]/div[4]/div[3]/div[2]/ul[1]/li/a[1]'
        url_nodes = await cat_page.xpath(subcategory_url_xpath)
        # Retrieve URLs from those nodes
        sub_cat_urls = list()
        for node in url_nodes:
            sub_url = await cat_page.evaluate('(node) => node.href', node)
            sub_cat_urls.append(sub_url)
        # Close category page
        await cat_page.close()
        return sub_cat_urls

    @staticmethod
    async def get_num_of_apps(sub_cat_page) -> int:
        num_apps_xpath = '/html/body/form/div[4]/div[3]/h2/span'
        num_apps_element = await sub_cat_page.xpath(num_apps_xpath)
        node = num_apps_element[0]
        raw_string = await sub_cat_page.evaluate('(node) => node.textContent', node)
        num_apps = int(re.findall('[0-9]+', raw_string)[0])
        return num_apps

    @staticmethod
    async def retrieve_app_urls(app_page) -> list:
        app_url_xpath = '/html[1]/body[1]/form[1]/div[4]/div[3]/div/table[1]/tbody[1]/tr/td[4]/a[1]'
        app_nodes = await app_page.xpath(app_url_xpath)
        app_urls = list()
        for node in app_nodes:
            app_url = await app_page.evaluate('(node) => node.href', node)
            app_urls.append(app_url)
        return app_urls

    @staticmethod
    def delete_temporary_files(category_path):
        """Deleting temporary, unfinished downloading files
        (which have .crdownload extension) in data directory.
        """
        # Scan and get the list of all .crdownload file paths
        all_temp_paths = list(Path(category_path).glob('*.crdownload'))
        # Delete temporary files based on their paths detected
        for temp_path in all_temp_paths:
            Path(temp_path).unlink(missing_ok=True)

    async def crawl_data_in_subcategory(self, category_browser, sub_cat_url, category_path, starting_page: int = 1):
        # Open new sub-category page
        sub_cat_page = await self.open_page(category_browser, sub_cat_url)
        # Get total number of apps in the sub-category
        total_apps_num = await self.get_num_of_apps(sub_cat_page)
        num_of_pages = math.ceil(total_apps_num / 10)
        await sub_cat_page.close()
        # Process each page and get 10 app URLs of each
        for page_num in range(starting_page, num_of_pages+1):
            page_url = sub_cat_url + '/' + str(page_num)
            app_page = await self.open_page(category_browser, page_url)
            app_urls = await self.retrieve_app_urls(app_page)
            await app_page.close()
            # Process and download 10 apps
            await asyncio.gather(*[self.process_n_download_app_data(category_browser, app_url, category_path)
                                   for app_url in app_urls])

    @staticmethod
    async def retrieve_os_info(app_page):
        os_xpath = '/html/body/form/div[4]/div/div[1]/ul/li[7]/span'
        os_nodes = await app_page.xpath(os_xpath)
        try:
            os_node = os_nodes[0]
            os_info = await app_page.evaluate('(os_node) => os_node.textContent', os_node)
            return os_info
        except Exception as e:
            # This exception is used to handle pages that OS info is hidden
            return ''

    @staticmethod
    async def get_download_url_n_file_name(app_page):
        """Retrieve downloadable URL and download file name of the app."""
        download_file_xpath = '/html/body/form/div/div/div/div/div/div[1]/a'
        download_file_nodes = await app_page.xpath(download_file_xpath)
        try:
            download_file_node = download_file_nodes[0]
            download_url = await app_page.evaluate('(download_file_node) => download_file_node.href',
                                                   download_file_node)
            file_name = download_url.split('/')[-1]
            return download_url, file_name
        except Exception as e:
            # This exception is used to handle pages where download button is hidden
            return 'about:blank', ''

    @staticmethod
    def is_file_exist(category_path, file_name) -> bool:
        file_path = Path(category_path)/Path(file_name)
        return Path(file_path).exists()

    @staticmethod
    async def download_app(app_page, download_url, is_file_exists: bool):
        """Only download file if the file is not exist in the directory.
        This avoids re-downloading files that leads to duplicate files.
        """
        if not is_file_exists:
            try:
                await app_page.goto(download_url)
            except Exception as e:
                # Set download _timeout for 30 seconds for downloadable links
                if str(e).find('ERR_ABORTED') > -1:
                    await app_page.waitFor(30000)
                # Skip if the link died
                else:
                    await asyncio.sleep(0.1)

    async def process_n_download_app_data(self, category_browser, app_url, category_path):
        """Retrieving app information, and download app files."""
        # Open the application page
        app_page = await self.open_page(category_browser, app_url)
        # Set download location to the category path
        await app_page._client.send('Page.setDownloadBehavior', {'behavior': 'allow',
                                                                 'downloadPath': str(category_path)})
        # Retrieve supporting OS(es) of the app
        support_os = await self.retrieve_os_info(app_page)
        # Retrieve download URL of the app, as well as its file name
        download_url, file_name = await self.get_download_url_n_file_name(app_page)
        # Generate Windows OS supporting flag
        windows_support_flag = support_os.find('Windows') > -1
        # Only download apps that supports Windows OS
        if windows_support_flag:
            file_exists = self.is_file_exist(category_path, file_name)
            await self.download_app(app_page, download_url, file_exists)
        # Close the page
        await app_page.close()

    async def crawl_data_in_category(self, cat_url, category_path, subcategory_index: int = -1, starting_page: int = 1):
        # Open new window for new category
        cat_browser = await self.launch_browser()
        # Retrieve list of URLs of sub-categories
        sub_cat_urls = await self.retrieve_subcategory_urls(cat_browser, cat_url)
        # If the category does not contain any sub-categories, use category URL as the only sub-category URL
        if len(sub_cat_urls) == 0:
            sub_cat_urls.append(cat_url)
        # If a sub-category index is passed, process and crawl data only
        # for corresponding sub-category
        if subcategory_index > -1:
            try:
                subcategory_url = sub_cat_urls[subcategory_index]
                await self.crawl_data_in_subcategory(cat_browser, subcategory_url, category_path, starting_page)
            except IndexError:
                print('Error: Cannot retrieve corresponding subcategory. Subcategory index invalid.')
        # Otherwise, process and crawl data in every sub-category
        else:
            for sub_cat_url in sub_cat_urls:
                await self.crawl_data_in_subcategory(cat_browser, sub_cat_url, category_path, starting_page)
        # Close the window when processing complete
        await cat_browser.close()
        # Clean up and delete any temporary files (.crdownload files)
        # in the directory after processing
        self.delete_temporary_files(category_path)

    async def get_category_n_url_lists(self):
        # Open new browser window, and get the category-url data
        init_browser = await self.launch_browser()
        await self.get_categories_n_urls(init_browser)
        # Preprocessing the category-url data, and get URL and category lists
        self.preprocessing_category_url_dictionary()
        url_list = list(self.category_url_dict.values())
        category_list = list(self.category_url_dict.keys())
        # Close the init window
        await init_browser.close()
        return url_list, category_list

    def create_category_paths(self, category_list: list) -> list:
        """Make folders for categories, as well as returning their paths"""
        raw_data_directory = self.get_raw_data_directory()
        category_paths = self.make_subdirectories(raw_data_directory, category_list)
        return category_paths

    async def crawl_all_data(self):
        # Process home page of the target website
        url_list, category_list = await self.get_category_n_url_lists()
        category_paths = self.create_category_paths(category_list)
        # Process each category in a browser, one category at a time
        for category_url, category_path in zip(url_list, category_paths):
            await self.crawl_data_in_category(category_url, category_path)

    async def crawl_data_of_a_subcategory(self, category_index: int, subcategory_index: int, starting_page: int = 1):
        """The base is same as crawl all data function, but only crawl data
        based on index parameters.
        """
        # Process home page of the target website
        url_list, category_list = await self.get_category_n_url_lists()
        category_paths = self.create_category_paths(category_list)
        # Crawl data based on category and sub-category indexes
        try:
            category_url = url_list[category_index]
            category_path = category_paths[category_index]
            await self.crawl_data_in_category(category_url, category_path, subcategory_index, starting_page)
        except IndexError:
            print('Error: Cannot retrieve corresponding category. Category index invalid')
